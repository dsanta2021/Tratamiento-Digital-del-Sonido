{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 6 Tecnologías del Habla\n",
    "\n",
    "\n",
    "### Rebeca Goya Esteban y Óscar Barquero Pérez\n",
    "\n",
    "update: 15 de noviembre de 2022\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Licencia de Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />Este obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">licencia de Creative Commons Reconocimiento-NoComercial-CompartirIgual 4.0 Internacional</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Clasificadores Máquina\n",
    "\n",
    "En este notebook vamos a aplicar tres técnicas de aprendizaje máquina para resolver un problema de clasificación.\n",
    "\n",
    "**Descripción de los datos**: The iris data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
    "\n",
    "\n",
    "En la siguiente celda se cargan los datos y se realiza una división de los mismos en un conjunto de entrenamiento y un conjunto de test. En *X_train* y *X_test* corresponden a carcterı́sticas de tres clases de flores, en total hay 150\n",
    "ejemplos. En *y_train* y *y_test* se encuentra la información de a qué clase\n",
    "pertenece cada una de las 150 flores (tres posibles clases).\n",
    "\n",
    "**¿Cuántas características se están utilizando para representar/describir a cada una de las flores?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#imports\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "#load datasets and split into test and train\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "#30% de test\n",
    "\n",
    "#split train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar el Método de Clasificación de [Naïve Bayes GaussianNB](http://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes), donde hay dos asunciones básicas:\n",
    "\n",
    "* Asunción *naïve* de independencia condicional entre las características para una clase de la variable respuesta dada:\n",
    "\n",
    "$$P(x_1,\\ldots,x_d\\mid y_i) = \\prod_{k = 1}^{d}P(x_k\\mid y_i)$$\n",
    "\n",
    "* La verosimilitud de cada una de las características se asume Gaussiana:\n",
    "\n",
    "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "A continuación entrenamos el algoritmo, lo que implica calcular los parámetros $\\mu_y$ y $\\sigma_y$.\n",
    "\n",
    "* Utilice los datos de entrenamiento (*X_Train* y *y_Train*) y la función *GaussianNB.fit()* para estimar los parámetros de la pdf gaussiana de cada una de las 3 clases de flores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Creamos el modelo de NB\n",
    "\n",
    "model_NB = GaussianNB()\n",
    "\n",
    "#entrenamos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de la gaussiana para cada clase:\n",
      " Columnas = Características \n",
      " Filas = Clases\n",
      "\n",
      "[[5.01034483 3.42758621 1.47931034 0.25862069]\n",
      " [5.97317073 2.7804878  4.29756098 1.33414634]\n",
      " [6.64857143 2.95428571 5.66       2.02571429]]\n"
     ]
    }
   ],
   "source": [
    "# Media y desviación estándar para las características de cada clase:\n",
    "\n",
    "print(\"Media de la gaussiana para cada clase:\\n Columnas = Características \\n Filas = Clases\\n\")\n",
    "print(model_NB.theta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desviación estándar de la gaussiana para cada clase:\n",
      " Columnas = Características \n",
      " Filas = Clases\n",
      "\n",
      "[[0.11265161 0.11785969 0.03129608 0.01208086]\n",
      " [0.28391434 0.10498513 0.20267698 0.04078525]\n",
      " [0.3813551  0.09791021 0.29325715 0.08076735]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Desviación estándar de la gaussiana para cada clase:\\n Columnas = Características \\n Filas = Clases\\n\")\n",
    "print(model_NB.sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "* Clasifique cada ejemplo (flor) de los datos de test (x test iris y y test iris) de acuerdo al esquema de clasificación Naive Bayes. Para ello, utilice *model_NB.predict(X_test)*\n",
    "* Calcule el error de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Predecimos las muestras de test, utilizando los valores de la función estimados anteriorment en train\n",
    "\n",
    "y_hat_NB = \n",
    "\n",
    "#Calculamos el accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Ejemplo KNN con sklearn\n",
    "\n",
    "Vamos a utilizar el método de clasificación KNN https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "**Pruebe diferentes valores para el parámetro K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "model_KNN = \n",
    "\n",
    "#train the model \n",
    "\n",
    "\n",
    "#predict outputs\n",
    "y_hat_KNN = \n",
    "\n",
    "#accuracy\n",
    "\n",
    "acc = \n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3 redes neuronales\n",
    "\n",
    "Vamos a utilizar el método de clasificación redes neuronales https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "\n",
    "**Pruebe diferentes configuraciones de capas y neuronas ocultas**\n",
    "\n",
    "**Pruebe o modificar otros parámetros de la red neuronal**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeca/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_RN =\n",
    "\n",
    "#train the model \n",
    "\n",
    "\n",
    "#predict outputs\n",
    "y_hat_RN = \n",
    "\n",
    "#accuracy\n",
    "\n",
    "acc = \n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
